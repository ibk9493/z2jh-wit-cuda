FROM nvidia/cuda:11.5.2-cudnn8-devel-ubuntu20.04

ENV DEBIAN_FRONTEND=noninteractive

USER root

# Create a user for app
RUN groupadd -g 3030 jupyter
RUN adduser --disabled-password -uid 3030 -gid 3030 --gecos "Default user" --home /home/jupyter --force-badname jupyter

# Create a user for JupyterHub
RUN echo "jupyter:admin" | chpasswd

# Create a directories for JupyterHub
RUN mkdir -p /etc/jupyterhub /jupyterhub_data /home/jupyter /usr/local/lib/python3.8/dist-packages /home/jupyter/.local/lib/python3.8/site-packages && \
    chown -R jupyter:jupyter /jupyterhub_data /home/jupyter && \
    chmod 777 /usr/local/lib/python3.8/dist-packages && \
    chmod 000 /home/jupyter/.local/lib/python3.8/site-packages
# RUN chown -R jupyter:jupyter /jupyterhub_data 

# Create directories for spark
RUN mkdir -p /opt/spark/jars /opt/spark/examples /opt/spark/work-dir /opt/sparkRapidsPlugin /opt/spark/python
RUN chmod g+w /opt/spark/work-dir
RUN touch /opt/spark/RELEASE

# Configure PAM for wheel group
RUN echo "auth required pam_wheel.so use_uid" >> /etc/pam.d/su && \
    chgrp root /etc/passwd && chmod ug+rw /etc/passwd

# Install necessary packages
RUN apt-get update && \
    apt-get install -y software-properties-common && \
    add-apt-repository ppa:ubuntu-toolchain-r/test && \
    add-apt-repository 'deb http://dk.archive.ubuntu.com/ubuntu/ xenial main' && \
    add-apt-repository 'deb http://dk.archive.ubuntu.com/ubuntu/ xenial universe' && \
    apt-get install -y --no-install-recommends openjdk-8-jdk openjdk-8-jre && \
    apt-get install -y --allow-downgrades --allow-change-held-packages --no-install-recommends g++-4.9 && \
    apt-get install -y build-essential libffi-dev libpq-dev libzmq3-dev python3-pip wget gnupg2 openssh-client openssh-server && \
    ln -s /usr/bin/python3 /usr/bin/python

# Update SSH configuration
RUN apt-get clean && rm -rf /var/lib/apt/lists/* && \
    mkdir -p /var/run/sshd && \
    grep -v StrictHostKeyChecking /etc/ssh/ssh_config > /etc/ssh/ssh_config.new && \
    echo "    StrictHostKeyChecking no" >> /etc/ssh/ssh_config.new && \
    mv /etc/ssh/ssh_config.new /etc/ssh/ssh_config

# Download and extract Hadoop
RUN wget https://archive.apache.org/dist/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz && \
    tar xzf hadoop-2.7.7.tar.gz && \
    rm -f hadoop-2.7.7.tar.gz

#Setup ENVs
ENV HADOOP_HOME /hadoop-2.7.7
ENV SPARK_HOME /opt/spark
ENV JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk-amd64
ENV PATH $PATH:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin:/opt/spark/bin:/opt/spark/bin:/usr/local/lib/python3.8/dist-packages

# Copy Spark files and configurations
COPY spark-3.1.1-bin-hadoop2.7/getGpusResources.sh /
COPY spark-3.1.1-bin-hadoop2.7/getGpusResources.sh /opt/sparkRapidsPlugin
COPY spark-3.1.1-bin-hadoop2.7/bin /opt/spark/bin
COPY spark-3.1.1-bin-hadoop2.7/sbin /opt/spark/sbin
COPY spark-3.1.1-bin-hadoop2.7/data /opt/spark/data
COPY spark-3.1.1-bin-hadoop2.7/python/pyspark /opt/spark/python/pyspark
COPY spark-3.1.1-bin-hadoop2.7/python/lib /opt/spark/python/lib
COPY spark-3.1.1-bin-hadoop2.7/conf /opt/spark/conf

# install requirements for https
RUN apt-get update
RUN apt-get install -y apt-utils nodejs npm 
RUN npm install -g configurable-http-proxy

# create and add rules for jupyteruser
RUN usermod -aG sudo jupyter
RUN touch /jupyterhub_cookie_secret
RUN chown jupyter:jupyter /jupyterhub_cookie_secret
RUN chmod 600 /jupyterhub_cookie_secret
RUN echo "c.JupyterHub.db_url = 'sqlite:////jupyterhub_data/jupyterhub.sqlite'" >> /etc/jupyterhub/jupyterhub_config.py


RUN chmod 777 /usr/lib/python3/dist-packages/pip

# Copy and install base requirements for JupyterHub
COPY base_req.txt requirements.txt
RUN pip install build && \
    pip wheel -r requirements.txt && \
    pip install -r requirements.txt
    
RUN jupyterhub upgrade-db
RUN echo "c.JupyterHub.cookie_secret_file = '/jupyterhub_data/jupyterhub_cookie_secret'" >> /etc/jupyterhub/jupyterhub_config.py
# Copy and install requirements for add on packages
COPY requirements.txt all_requirements.txt      

RUN pip install -r all_requirements.txt

COPY spark-3.1.1-bin-hadoop2.7/jars /opt/spark/jars

# Expose JupyterHub port
USER jupyter
WORKDIR /home/jupyter

EXPOSE 8000

# Set the entry point
CMD ["jupyterhub"]
